?Corpus
docs <- tm_map(docs, removePunctuation)
View(docs)
d.corpus <- tm_map(d.corpus, removeNumbers)
t <- tm_map(docs, removeNumbers)
docs[["1"]][["content"]]
View(t)
text <- c("我是，1片雲", "Sarah likes me, but I hate her!")
t.corp <- Corpus(VectorSource(text))
t.corp <- tm_map(t.corp, removeNumbers)
t.corp <- tm_map(t.corp, removePunctuation)
text <- c()
View(t.corp)
text <- c("我是，1片雲", "Sarah likes me, but I hate her!")
t.corp <- Corpus(VectorSource(text))
t.corp <- tm_map(t.corp, removeNumbers)
t.corp <- tm_map(t.corp, removePunctuation)
docs <- tm_map(docs, removeNumbers) #remove numbers
fin <- file("stopwords_tw.txt")
typeof(fin)
?readLines
fin <- file("stopwords_tw.txt", open = "r")
file
?file
words <- readLines("Machine-Learning/data/停用詞-繁體中文.txt")
words <- readLines("https://github.com/tomlinNTUB/Machine-Learning/blob/master/data/%E5%81%9C%E7%94%A8%E8%A9%9E-%E7%B9%81%E9%AB%94%E4%B8%AD%E6%96%87.txt")
words <- toTrad(words)
insertWords(words)
stopwords <- readLines("https://github.com/tomlinNTUB/Machine-Learning/blob/master/data/%E5%81%9C%E7%94%A8%E8%A9%9E-%E7%B9%81%E9%AB%94%E4%B8%AD%E6%96%87.txt")
stopwords <- toTrad(words)
insertWords(stopwords)
docs <- tm_map(d.corpus, removeWords, stopwords)
docs <- tm_map(docs, removeWords, stopwords)
insertWords()
?insertWords
mixseg = worker()
# segment <- c("詹順貴","深澳電廠","環評","新北市")
# new_user_word(mixseg,segment)
cutted.post <- segment(t.docs, mixseg)
# segment <- c("詹順貴","深澳電廠","環評","新北市")
# new_user_word(mixseg,segment)
cutted.post <- segment(docs, mixseg)
#斷詞，stopwords
docs <- unlist(docs)
mixseg = worker()
# segment <- c("詹順貴","深澳電廠","環評","新北市")
# new_user_word(mixseg,segment)
cutted.post <- segment(docs, mixseg)
View(cutted.post)
worker()
?worker
stopwords <- readLines("https://github.com/tomlinNTUB/Machine-Learning/blob/master/data/%E5%81%9C%E7%94%A8%E8%A9%9E-%E7%B9%81%E9%AB%94%E4%B8%AD%E6%96%87.txt")
s <- readLines("https://github.com/tomlinNTUB/Machine-Learning/blob/master/data/%E5%81%9C%E7%94%A8%E8%A9%9E-%E7%B9%81%E9%AB%94%E4%B8%AD%E6%96%87.txt")
stopwords<-c(NULL)
for(i in 1:length(s))
for(i in 1:length(s))
{
stopwords[i]<-s[i]
}
test <-filter_segment(cutted.post,stopwords)#去除中文停止词
View(stopwords)
s <- readLines("https://github.com/tomlinNTUB/Machine-Learning/blob/master/data/%E5%81%9C%E7%94%A8%E8%A9%9E-%E7%B9%81%E9%AB%94%E4%B8%AD%E6%96%87.txt"
, encoding="UTF-8")
stopwords<-c(NULL)
for(i in 1:length(s))
for(i in 1:length(s))
{
stopwords[i]<-s[i]
}
View(stopwords)
s <- readLines("stopwords_tw.txt"
, encoding="UTF-8")
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
}
#要清洗掉的東西
docs <- Corpus(VectorSource(zhan.post))
#定義清洗：清洗就是把你找到的符號用空白取代
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
})
#要清洗掉的東西
docs <- Corpus(VectorSource(zhan.post))
#定義清洗：清洗就是把你找到的符號用空白取代
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
})
docs <- tm_map(docs,toSpace,"V1")
docs <- tm_map(docs,toSpace,"\n")
docs <- tm_map(docs,toSpace, "1")
docs <- tm_map(docs,toSpace, "的")
docs <- tm_map(docs,toSpace, "及")
docs <- tm_map(docs,toSpace, "為")
docs <- tm_map(docs,toSpace, "是")
docs <- tm_map(docs,toSpace, "在")
docs <- tm_map(docs,toSpace, "[A-Za-z0-9]")
docs <- tm_map(docs, removeNumbers) #remove numbers
docs <- tm_map(docs, stripWhitespace)#清除空格
#要清洗掉的東西
docs <- Corpus(VectorSource(zhan.post))
#定義清洗：清洗就是把你找到的符號用空白取代
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
})
docs <- tm_map(docs,toSpace,"V1")
docs <- tm_map(docs,toSpace,"\n")
docs <- tm_map(docs,toSpace, "1")
docs <- tm_map(docs,toSpace, "的")
docs <- tm_map(docs,toSpace, "及")
docs <- tm_map(docs,toSpace, "為")
docs <- tm_map(docs,toSpace, "是")
docs <- tm_map(docs,toSpace, "在")
docs <- tm_map(docs,toSpace, "才")
docs <- tm_map(docs,toSpace, "所以")
docs <- tm_map(docs,toSpace, "是否")
docs <- tm_map(docs,toSpace, "再")
docs <- tm_map(docs,toSpace, "與")
docs <- tm_map(docs,toSpace, "等")
docs <- tm_map(docs,toSpace, "這")
docs <- tm_map(docs,toSpace, "或")
docs <- tm_map(docs,toSpace, "[A-Za-z0-9]")
docs <- tm_map(docs, removeNumbers) #remove numbers
docs <- tm_map(docs, stripWhitespace)#清除空格
#docs <- tm_map(docs, removePunctuation) #removePunc
mixseg = worker()
segment <- c("阿貴","深澳","瑞芳","環差","新北","台北")
new_user_word(mixseg,segment)
docs <- Corpus(VectorSource(zhan.post))
#定義清洗：清洗就是把你找到的符號用空白取代
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
})
#用tm_map斷詞
myStopWords <- c(stopwordsCN(), "的", "及", "與", "為", "等", "所以", "才", "再")
docs <- tm_map(docs, removeWords, myStopWords)
docs <- tm_map(docs,toSpace,"V1")
docs <- tm_map(docs,toSpace,"\n")
docs <- tm_map(docs,toSpace, "1")
docs <- tm_map(docs,toSpace, "[A-Za-z0-9]")
docs <- tm_map(docs, removeNumbers) #remove numbers
docs <- tm_map(docs, stripWhitespace)#清除空格
View(docs)
mixseg = worker()
segment <- c("阿貴","深澳","瑞芳","環差","新北","台北")
new_user_word(mixseg,segment)
cutted.post <- segment(docs, mixseg)
cutted.post <- segment(docs, worker)
cutted.post <- segment(docs, worker)
#要清洗掉的東西
docs <- Corpus(VectorSource(zhan.post))
#用tm_map斷詞
myStopWords <- c(stopwordsCN(), "的", "及", "與", "為", "等", "所以", "才", "再")
docs <- tm_map(docs, removeWords, myStopWords)
#定義清洗：清洗就是把你找到的符號用空白取代
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
})
docs <- tm_map(docs,toSpace,"V1")
docs <- tm_map(docs,toSpace,"\n")
docs <- tm_map(docs,toSpace, "1")
# docs <- tm_map(docs,toSpace, "是")
# docs <- tm_map(docs,toSpace, "在")
# docs <- tm_map(docs,toSpace, "才")
# docs <- tm_map(docs,toSpace, "所以")
# docs <- tm_map(docs,toSpace, "是否")
# docs <- tm_map(docs,toSpace, "再")
# docs <- tm_map(docs,toSpace, "與")
# docs <- tm_map(docs,toSpace, "等")
# docs <- tm_map(docs,toSpace, "這")
# docs <- tm_map(docs,toSpace, "或")
docs <- tm_map(docs,toSpace, "[A-Za-z0-9]")
docs <- tm_map(docs, removeNumbers) #remove numbers
docs <- tm_map(docs, stripWhitespace)#清除空格
View(docs)
docs <- tm_map(docs,toSpace, "的")
docs <- tm_map(docs,toSpace, "及")
docs <- tm_map(docs,toSpace, "為")
docs <- tm_map(docs,toSpace, "是")
docs <- tm_map(docs,toSpace, "在")
docs <- tm_map(docs,toSpace, "才")
docs <- tm_map(docs,toSpace, "[A-Za-z0-9]")
docs <- tm_map(docs, removeNumbers) #remove numbers
docs <- tm_map(docs, stripWhitespace)#清除空格
?content_transformer
t.corp <- tm_map(t.corp, toSpace, "是")
docs <- tm_map(docs,toSpace, "是")
View(t.corp)
View(docs)
docs <- tm_map(docs,toSpace,"V1")
docs <- tm_map(docs,toSpace,"\n")
docs <- tm_map(docs,toSpace, "1")
docs <- tm_map(docs,toSpace, "的")
docs <- tm_map(docs,toSpace, "及")
docs <- tm_map(docs,toSpace, "為")
docs <- tm_map(docs,toSpace, "是")
docs <- tm_map(docs,toSpace, "在")
docs <- tm_map(docs,toSpace, "才")
# docs <- tm_map(docs,toSpace, "所以")
# docs <- tm_map(docs,toSpace, "是否")
# docs <- tm_map(docs,toSpace, "再")
# docs <- tm_map(docs,toSpace, "與")
# docs <- tm_map(docs,toSpace, "等")
# docs <- tm_map(docs,toSpace, "這")
# docs <- tm_map(docs,toSpace, "或")
docs <- tm_map(docs,toSpace, "[A-Za-z0-9]")
docs <- tm_map(docs, removeNumbers) #remove numbers
docs <- tm_map(docs, stripWhitespace)#清除空格
View(docs)
docs <- tm_map(docs,toSpace, "才")
docs <- tm_map(docs,toSpace, "所以")
docs <- tm_map(docs,toSpace, "是否")
docs <- tm_map(docs,toSpace, "再")
docs <- tm_map(docs,toSpace, "與")
docs <- tm_map(docs,toSpace, "等")
docs <- tm_map(docs,toSpace, "這")
docs <- tm_map(docs,toSpace, "或")
docs <- tm_map(docs,toSpace, "[A-Za-z0-9]")
docs <- tm_map(docs,toSpace, "，")
docs <- tm_map(docs,toSpace, "。")
docs <- tm_map(docs,toSpace, "？")
docs <- tm_map(docs,toSpace, "！")
View(docs)
docs <- tm_map(docs, stripWhitespace)#清除空格
docs <- tm_map(docs, removePunctuation) #removePunctuation移除的是半形標點符號
docs
View(docs)
mixseg = worker()
segment <- c("阿貴","深澳","瑞芳","環差","新北","台北")
new_user_word(mixseg, segment)
cutted.post <- segment(docs, worker)
mixseg = worker()
segment <- c("阿貴","深澳","瑞芳","環差","新北","台北")
new_user_word(mixseg, segment)
cutted.post <- segment(docs, mixseg)
suppressMessages(library(randomForest))
suppressMessages(library(dplyr))
suppressMessages(library(rpart))
suppressMessages(library(party))
# For exploration check my other kernel
# this one I will focus on pass the 0.81 score mark
install.packages("party")
suppressMessages(library(party))
# for validate missing data
suppressMessages(library(mice))
suppressMessages(library(VIM))
suppressMessages(library(FSelector))
install.packages("FSelector")
# for validate missing data
suppressMessages(library(mice))
suppressMessages(library(VIM))
suppressMessages(library(FSelector))
suppressMessages(library(randomForest))
suppressMessages(library(dplyr))
suppressMessages(library(rpart))
suppressMessages(library(party))
# for validate missing data
suppressMessages(library(mice))
suppressMessages(library(VIM))
suppressMessages(library(FSelector))
suppressMessages(library(randomForest))
suppressMessages(library(dplyr))
suppressMessages(library(rpart))
suppressMessages(library(party))
# for validate missing data
suppressMessages(library(mice))
suppressMessages(library(VIM))
suppressMessages(library(FSelector))
# for validate missing data
suppressMessages(library(mice))
install.packages("mice")
# for validate missing data
suppressMessages(library(mice))
install.packages("VIM")
suppressMessages(library(VIM))
suppressMessages(library(FSelector))
install.packages("FSelector")
# load train.csv
DstTrain <- read.csv('../input/train.csv', stringsAsFactors = FALSE, na.strings = c("NA", ""))
test2 <- load("/data.RData",open="r")
test2 <- load("/data.RData")
test2 <- load("data.RData")
load("data.Rdata")
load("data.Rdata")
load("~/Documents/天下工讀事項/總統盃黑客松/data.RData")
load("~/Documents/天下工讀事項/總統盃黑客松/data.RData", ex <- new.env())
ls.str(ex)
View(ex)
library(Hmisc)
library(knitr)
library(ggplot2)
library(Hmisc)
library(knitr)
library(ggplot2)
library(dplyr)
library(caret)
library(randomForest)
library(gridExtra)
library(ROCR)
library(corrplot)
install.packages("Hmisc")
install.packages("caret")
install.packages("gridExtra")
install.packages("ROCR")
install.packages("corrplot")
library(Hmisc)
library(caret)
library(gridExtra)
library(ROCR)
library(corrplot)
?readLines
?read.table
?readLines
?strsplit
?group_by
rm(list = ls())
library(httr)
library(rvest)
#爬柯文哲五月新聞的網頁網址
ltn_may_url <- "http://news.ltn.com.tw/search/?keyword=%E6%9F%AF%E6%96%87%E5%93%B2&conditions=and&SYear=2018&SMonth=5&SDay=1&EYear=2018&EMonth=5&EDay=31&page="
may_ltnurl <- c()
#55頁只是5/1-5/31
for( i in 1: 55) {
print(i)
url <- paste0(ltn_may_url , i )
may_ltnurl <- rbind(may_ltnurl , url )
Sys.sleep(sample(1:5 , 1))
}
may_ltnurl[1]
tdoc <- read_html(may_ltnurl[1])
css <- "#newslistul > li > a.tit"
node <- html_nodes(tdoc, css)
tlink <- html_attrs(node)
t <- unlist(tlink)
t <- matrix(t, ncol = 3, byrow = T)
t <- t[ , 2]
pre <- "http://ent.ltn.com.tw/"
ta <- c()
for (i in 1:length(t)) {
print(i)
tl <- paste0(pre , t[i])
ta <- rbind(ta, tl)
}
View(ta[1])
tadoc <- read_html(ta[1])
ticss <- "main > span > div.content > div.news_content > h1"# why 沒東西ＱＱ
tinode <- html_node(tadoc , "main > span > div.content > div.news_content > h1")
tinode <- html_node(tadoc , xpath = '//*[@id="main"]/span/div[1]/div[2]/h1')
ti <- html_text(tinode)
ticss <- "#main > span > div.content > div.news_content > h1"# why 沒東西ＱＱ
tinode <- html_node(tadoc , ticss)
ta[1]
tadoc <- read_html(ta[1])
tadoc <- read_html(ta[1])
ticss <- "#main > span > div.content > div.news_content > h1"# why 沒東西ＱＱ
tinode <- html_node(tadoc , ticss)
ti <- html_text(tinode)
tinode <- html_nodes(tadoc , ticss)
rm(list = ls())
library(RSelenium)
# 連接 Selenium 伺服器，選用 Firefox 瀏覽器
remDr <- remoteDriver(
remoteServerAddr = "localhost",
port = 4444,
browserName = "chrome")
# 開啟瀏覽器
remDr$open()
# 連接 Selenium 伺服器，選用 Firefox 瀏覽器
remDr <- remoteDriver(
remoteServerAddr = "localhost",
port = 4444,
browserName = "chrome")
# 開啟瀏覽器
remDr$open()
library(RSelenium)
# 連接 Selenium 伺服器，選用 Firefox 瀏覽器
remDr <- remoteDriver(
remoteServerAddr = "localhost",
port = 4444,
browserName = "chrome")
# 開啟瀏覽器
remDr$open()
# 瀏覽 Google 首頁
remDr$navigate("https://www.google.com.tw/")
# 瀏覽 Yahoo 首頁
remDr$navigate("https://tw.yahoo.com/")
# 回到上一頁
remDr$goBack()
# 前往下一頁
remDr$goForward()
# 取的目前網頁的網址
remDr$getCurrentUrl()
# 重新整理
remDr$refresh()
# 用 Google 搜尋 office 關鍵字
remDr$navigate("https://www.google.com.tw/search?q=office")
library(httr)
library(rvest)
library(magrittr)
library(XML)  # readHTMLTable
library(dplyr) # data manipulation & pipe line
library(stringr)
res <- POST(
"https://tw.appledaily.com/appledaily/search",
body = "searchType=text&searchMode=Sim&querystrS=%E6%9F%AF%E6%96%87%E5%93%B2",
encode = "form")
print(res)
library(rvest)
url <- "https://tw.appledaily.com/search"
form <- list(searchType = "text",
keyword = "柯文哲",
totalpage = "25418",
page = "1",
sorttype = "1",
rangedate = "[20030502 TO 20171112999:99]")
res <- POST(url, dody = form)
doc.str <- content(res, "text")
doc <- read_html(doc.str)
doc <- read_html(doc.str)
doc.str <- content(res, "text")
doc <- read_html(doc.str)
res <- POST(url, body = form)
url <- "https://tw.appledaily.com/search"
form <- list(searchType = "text",
keyword = "柯文哲",
totalpage = "25418",
page = "1",
sorttype = "1",
rangedate = "[20030502 TO 20171112999:99]")
res <- POST(url, body = form)
doc.str <- content(res, "text")
doc <- read_html(doc.str)
View(doc)
css <- "#result li > div > h2 > a"
links <- html_attr(html_nodes(doc, css), "href")
links
library(jiebaR)
setwd("~/Documents/GitHub/courseR/final_project")
setwd("~/Documents/GitHub/courseR/final_project/NTUSD_traditional")
# positive: combine 正面情感词语（中文）,正面评价词语（中文）, ntusd-positive
posdict = read.csv("NTUSD_positive_unicode.txt", header = FALSE, stringsAsFactors = FALSE)[,1]
# positive: combine 正面情感词语（中文）,正面评价词语（中文）, ntusd-positive
posdict = read.csv("NTUSD_positive_unicode.txt", header = FALSE, stringsAsFactors = FALSE)[,1]
setwd("~/Documents/GitHub/courseR/final_project")
library(readr)
ntusd_pos <- read_csv("ntusd_pos.csv" , encoding = "Big5")
View(ntusd_pos)
setwd("~/Documents/GitHub/courseR/final_project")
# positive: combine 正面情感词语（中文）,正面评价词语（中文）, ntusd-positive
posdict = read.csv("ntusd_pos", header = FALSE, stringsAsFactors = FALSE , encoding = "Big5")[,1]
# positive: combine 正面情感词语（中文）,正面评价词语（中文）, ntusd-positive
posdict = read.csv("ntusd_pos.csv", header = FALSE, stringsAsFactors = FALSE , encoding = "Big5")[,1]
# positive: combine 正面情感词语（中文）,正面评价词语（中文）, ntusd-positive
posdict = read.csv("ntusd_pos.csv", encoding = "Big5")[,1]
# positive: combine 正面情感词语（中文）,正面评价词语（中文）, ntusd-positive
posdict = read.csv("ntusd_pos.csv", encoding = "Big5")
# negative: combine 负面情感词语（中文）,负面评价词语（中文）, ntusd-negative
negdict = read.csv("negative.txt", header = FALSE, stringsAsFactors = FALSE)[,1]
setwd("~/Documents/GitHub/courseR/final_project")
# positive: combine 正面情感词语（中文）,正面评价词语（中文）, ntusd-positive
posdict = read.csv("ntusd_pos.csv", encoding = "unicode")
# positive: combine 正面情感词语（中文）,正面评价词语（中文）, ntusd-positive
posdict = read.csv("ntusd_pos.csv", encoding = "unicode")
library(jiebaR)
# positive: combine 正面情感词语（中文）,正面评价词语（中文）, ntusd-positive
posdict = read.csv("ntusd_pos.csv", encoding = "unicode")
# positive: combine 正面情感词语（中文）,正面评价词语（中文）, ntusd-positive
posdict = read.csv("ntusd_pos.csv", encoding = "UTF-8")
# positive: combine 正面情感词语（中文）,正面评价词语（中文）, ntusd-positive
posdict = read.csv("ntusd_pos.csv", fileEncoding = "UTF-8")
# positive: combine 正面情感词语（中文）,正面评价词语（中文）, ntusd-positive
posdict = read.csv("ntusd_pos.csv", fileEncoding = "big5")
View(posdict)
unique(posdict)
str(posdict)
library(jiebaR)
# positive: combine 正面情感词语（中文）,正面评价词语（中文）, ntusd-positive
posdict = read.csv("ntusd_pos.csv", fileEncoding = "big5")
View(posdict)
unique(posdict, incomparables = TRUE)
unique(posdict, incomparables = False)
unique(posdict, incomparables = FALSE)
all <- read.csv("Yao_MayNews.csv")
setwd("~/Documents/GitHub/NTU-CSX-DataScience--Group5/Finalproject/UDN/爬完的結果!!")
all <- read.csv("Yao_MayNews.csv")
all <- read.csv("Yao_MayNews.csv" , fileEncoding = "big5")
View(all)
all <- read.csv("Yao_MayNews.csv" , fileEncoding = "Big5")
all <- read.csv("Yao_MayNews.csv" , encoding = "UTF-8")
setwd("~/Documents/GitHub/NTU-CSX-DataScience--Group5/Finalproject/UDN/爬完的結果!!")
all <- read.csv("Yao_MayNews.csv" , fileEncoding = "UTF-8")
all <- read.csv("Yao_MayNews.csv" )
all <- read.csv("Yao_MayNews.csv", fileEncoding = "big5")
all <- read.csv("Yao_MayNews.csv", fileEncoding = "UTF-8-BOM")
all <- read.csv("Yao_MayNews.csv", header = T ,fileEncoding = "UTF-8-BOM")
all <- read.csv(file = "Yao_MayNews.csv", header = T ,fileEncoding = "UTF-8-BOM")
all <- read.table(file = "Yao_MayNews.csv", header = T ,fileEncoding = "UTF-8-BOM")
View(all)
all <- read.csv(file = "Yao_MayNews.csv", header = T ,fileEncoding = "UTF-8-BOM")
all <- read.table(file = "Yao_MayNews.csv", header = T ,fileEncoding = "UTF-8-BOM")
View(all)
all <- read.csv(file = "Yao_MayNews.csv", header = T ,fileEncoding = "UTF-8-BOM")
setwd("~/Documents/GitHub/courseR")
all <- read.csv(file = "Yao1_MayNews.csv", header = T ,fileEncoding = "UTF-8-BOM")
View(all)
all <- read.csv(file = "Yao1_MayNews.csv" ,fileEncoding = "UTF-8")
all <- read.csv("Yao1_MayNews.csv", Encoding = "UTF-8")
all <- read.csv("Yao1_MayNews.csv", encoding = "UTF-8")
all <- read.csv("Yao1_MayNews.csv", encoding = "UTF-8")
all <- read.csv("Yao1_MayNews.csv")
all <- read.csv("Yao1_MayNews.csv")
View(all)
all <- read.csv("Yao1_MayNews.csv")
