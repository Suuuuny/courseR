#看topics_4的圖
dtm_topics_4 <- tidy(dtm_lda4)
top_terms_4 <- dtm_topics_4 %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms_4 %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
theme(axis.text.y=element_text(colour="black", family="Heiti TC Light"))
dtm_lda <- LDA(dtm, k = 8, control = list(seed = 1234)) #主題數4個
#畫圖
library(ggplot2)
dtm_topics <- tidy(dtm_lda, matrix = "beta")
top_terms <- dtm_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
# View(top_terms)
#畫圖
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
theme(axis.text.y=element_text(colour="black", family="Heiti TC Light"))
#載入套件
library(jiebaR)
library(dplyr)
library(topicmodels)
library(tidyr)
library(stringr)
library(tidytext)
#查看路徑
getwd()
#讀檔的地方
Ko_CTdoc <- read.csv("Ko_CTnews")
Ko_CTdoc <- na.omit(Ko_CTdoc)
#斷詞
cutter <- worker()
new_user_word(cutter, c("柯文哲","賴清德","丁守中","轉型正義","賴揆","習近平","林佳龍","呂秀蓮","涂醒哲", "世大運", "段宜康", "蔡英文", "馬英九", "韓國瑜" , "中執會" , "三立" , "中天", "姚文智" , "柯P" , "陳佩琪" , "台北市長", "前總統", "前副總統", "副總統" , "蔡璧如", "金溥聰"))
new_user_word(cutter, c("柯文哲","悠遊卡","陳景峻","賴清德","丁守中","轉型正義","賴揆","習近平","林佳龍","呂秀蓮","涂醒哲", "世大運", "段宜康", "蔡英文", "馬英九", "韓國瑜" , "中執會" , "三立" , "中天", "姚文智" , "柯P" , "陳佩琪" , "台北市長", "前總統", "前副總統", "副總統" , "蔡璧如", "金溥聰"))
Ko_CTdoc$words <- sapply(Ko_CTdoc$content %>% as.character() , function(x){tryCatch({cutter[x]}, error=function(err){})})
#讀取stop words檔
fin <- file("../Appnews/stopwords_tw.txt", open = "r")
stopwords <- readLines(fin , encoding = "UTF8")
stopwords <- c(stopwords,"表示", "報導")#stopwords 加上表示、報導
stopwords <- unique(stopwords) #刪去重複的stopwords
library(tidyr) # for unnest() 展開每一個被切開的詞
library(stringr)
word_token <- testdoc %>%
unnest() %>%
select(title, words) %>% #選新聞標題與切字的欄位
filter(!(words %in% stopwords)) %>%
filter(!str_detect(words, "\\d")) %>% #將某一些不符合正規表達式\\d的字挑掉
filter(nchar(words) > 1) #留下出現次數大於1的詞
word_token <- Ko_CTdoc %>%
unnest() %>%
select(title, words) %>% #選新聞標題與切字的欄位
filter(!(words %in% stopwords)) %>%
filter(!str_detect(words, "\\d")) %>% #將某一些不符合正規表達式\\d的字挑掉
filter(nchar(words) > 1) #留下出現次數大於1的詞
library(tidytext)
dtm <- word_token %>%
count(title, words) %>%
#count的語法是說數相同出現的詞出現幾次，然後最後會整理成有title,words和數數欄位的df
cast_dtm(title, words, n)
raw.sum=apply(dtm,1,FUN=sum) #sum by raw each raw of the table #統計每一列出現幾次詞
dtm=dtm[raw.sum!=0,] #將只有0的列刪掉
#弄lDA的模型
library(topicmodels)
dtm_lda <- LDA(dtm, k = 8, control = list(seed = 1234)) #主題數4個
#畫圖
library(ggplot2)
dtm_topics <- tidy(dtm_lda, matrix = "beta")
top_terms <- dtm_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
# View(top_terms)
#畫圖
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
theme(axis.text.y=element_text(colour="black", family="Heiti TC Light"))
dtm_lda <- LDA(dtm, k = 9, control = list(seed = 1234)) #主題數4個
#畫圖
library(ggplot2)
dtm_topics <- tidy(dtm_lda, matrix = "beta")
top_terms <- dtm_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
# View(top_terms)
#畫圖
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
theme(axis.text.y=element_text(colour="black", family="Heiti TC Light"))
#載入套件
library(jiebaR)
library(dplyr)
library(topicmodels)
library(tidyr)
library(stringr)
library(tidytext)
#查看路徑
getwd()
#讀檔的地方
Ding_CTdoc <- read.csv("Ding_CTnews")
Ding_CTdoc <- na.omit(Ding_CTdoc)
View(Ding_CTdoc)
#斷詞
cutter <- worker() #叫出斷詞函數
#加入自定義詞，使其不要被刪掉
new_user_word(cutter, c("吳茂昆","吳音寧","管中閔","拔管","挺管","","深澳","燃煤","蔣萬安","張顯耀","柯文哲","悠遊卡","陳景峻","賴清德","丁守中","轉型正義","賴揆","習近平","林佳龍","呂秀蓮","涂醒哲", "世大運", "段宜康", "蔡英文", "馬英九", "韓國瑜" , "中執會" , "三立" , "中天", "姚文智" , "柯P" , "陳佩琪" , "台北市長", "前總統", "前副總統", "副總統" , "蔡璧如", "金溥聰"))
Ding_CTdoc$words <- sapply(Ding_CTdoc$content %>% as.character() , function(x){tryCatch({cutter[x]}, error=function(err){})})
#讀取stop words檔
fin <- file("../Appnews/stopwords_tw.txt", open = "r")
stopwords <- readLines(fin , encoding = "UTF8")
stopwords <- c(stopwords,"表示", "報導")#stopwords 加上表示、報導
stopwords <- unique(stopwords) #刪去重複的stopwords
library(tidyr) # for unnest() 展開每一個被切開的詞
library(stringr)
word_token <- Ding_CTdoc %>%
unnest() %>%
select(title, words) %>% #選新聞標題與切字的欄位
filter(!(words %in% stopwords)) %>%
filter(!str_detect(words, "\\d")) %>% #將某一些不符合正規表達式\\d的字挑掉
filter(nchar(words) > 1) #留下出現次數大於1的詞
library(tidytext)
dtm <- word_token %>%
count(title, words) %>%
#count的語法是說數相同出現的詞出現幾次，然後最後會整理成有title,words和數數欄位的df
cast_dtm(title, words, n)
raw.sum=apply(dtm,1,FUN=sum) #sum by raw each raw of the table #統計每一列出現幾次詞
dtm=dtm[raw.sum!=0,] #將只有0的列刪掉
#弄lDA的模型
library(topicmodels)
dtm_lda <- LDA(dtm, k = 8, control = list(seed = 1234)) #主題數4個
#畫圖
library(ggplot2)
dtm_topics <- tidy(dtm_lda, matrix = "beta")
top_terms <- dtm_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
# View(top_terms)
#畫圖
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
theme(axis.text.y=element_text(colour="black", family="Heiti TC Light"))
dtm4_topics <- tidy(dtm_lda4, matrix = "beta")
#將dtm_lda轉換成tidy的df(???
#beta值是表示該主題是由多少比例的那個字所構成
top4_terms <- dtm4_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
#選出每一個topic的前十個數字
# View(top_terms)
#畫圖
top4_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
theme(axis.text.y=element_text(colour="black", family="Heiti TC Light"))
#畫主題數4個的
dtm4_topics <- tidy(dtm_lda4, matrix = "beta")
dtm_lda4 <- LDA(dtm, k = 4, control = list(seed = 1234)) #主題數4個
#畫主題數4個的
dtm4_topics <- tidy(dtm_lda4, matrix = "beta")
top4_terms <- dtm4_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
# View(top_terms)
#畫圖
top4_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
theme(axis.text.y=element_text(colour="black", family="Heiti TC Light"))
rm(list = ls())
#載入套件
library(jiebaR)
library(dplyr)
library(topicmodels)
library(tidyr)
library(stringr)
library(tidytext)
#查看路徑
getwd()
#斷詞
cutter <- worker() #叫出斷詞函數
#讀檔的地方
Yao_CTdoc <- read.csv("Yao_CTnews")
Yao_CTdoc <- na.omit(Yao_CTdoc)
#加入自定義詞，使其不要被刪掉
new_user_word(cutter, c("柯建銘","吳敦義","吳茂昆","吳音寧","管中閔","拔管","挺管","","深澳","燃煤","蔣萬安","張顯耀","柯文哲","悠遊卡","陳景峻","賴清德","丁守中","轉型正義","賴揆","習近平","林佳龍","呂秀蓮","涂醒哲", "世大運", "段宜康", "蔡英文", "馬英九", "韓國瑜" , "中執會" , "三立" , "中天", "姚文智" , "柯P" , "陳佩琪" , "台北市長", "前總統", "前副總統", "副總統" , "蔡璧如", "金溥聰"))
Yao_CTdoc$words <- sapply(Yao_CTdoc$content %>% as.character() , function(x){tryCatch({cutter[x]}, error=function(err){})})
#讀取stop words檔
fin <- file("../Appnews/stopwords_tw.txt", open = "r")
stopwords <- readLines(fin , encoding = "UTF8")
stopwords <- c(stopwords,"表示", "報導")#stopwords 加上表示、報導
stopwords <- unique(stopwords) #刪去重複的stopwords
word_token <- Ding_CTdoc %>%
unnest() %>%
select(title, words) %>% #選新聞標題與切字的欄位
filter(!(words %in% stopwords)) %>%
filter(!str_detect(words, "\\d")) %>% #將某一些不符合正規表達式\\d的字挑掉
filter(nchar(words) > 1) #留下出現次數大於1的詞
word_token <- Yao_CTdoc %>%
unnest() %>%
select(title, words) %>% #選新聞標題與切字的欄位
filter(!(words %in% stopwords)) %>%
filter(!str_detect(words, "\\d")) %>% #將某一些不符合正規表達式\\d的字挑掉
filter(nchar(words) > 1) #留下出現次數大於1的詞
library(tidytext)
dtm <- word_token %>%
count(title, words) %>%
#count的語法是說數相同出現的詞出現幾次，然後最後會整理成有title,words和數數欄位的df
cast_dtm(title, words, n)
raw.sum=apply(dtm,1,FUN=sum) #sum by raw each raw of the table #統計每一列出現幾次詞
dtm=dtm[raw.sum!=0,] #將只有0的列刪掉
dtm_lda <- LDA(dtm, k = 8, control = list(seed = 1234)) #主題數8個
#畫圖
library(ggplot2)
dtm_topics <- tidy(dtm_lda, matrix = "beta")
top_terms <- dtm_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
# View(top_terms)
#畫圖
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
theme(axis.text.y=element_text(colour="black", family="Heiti TC Light"))
new_stop_word <- c("柯建銘","吳敦義","吳茂昆","吳音寧","管中閔","拔管","挺管","","深澳","燃煤","蔣萬安","張顯耀","柯文哲","悠遊卡","陳景峻","賴清德","丁守中",
"轉型正義","賴揆","習近平","林佳龍","呂秀蓮","涂醒哲", "世大運", "段宜康", "蔡英文", "馬英九", "韓國瑜" , "中執會" , "三立" , "中天", "姚文智" ,
"柯P" , "陳佩琪" , "台北市長", "前總統", "前副總統", "副總統" , "蔡璧如", "金溥聰","台灣價值", "勤政清廉愛鄉土", "意識形態", "月票", "雙城論壇",
"陳昭南", "侯友宜", "重陽", "敬老", "建管處", "深澳電廠" , "陳彥伯", "燈節", "燈節廠商", "綠營", "藍營")
write(new_stop_words,file = "new_stop_words.txt",sep="\n")
new_stop_words <- c("柯建銘","吳敦義","吳茂昆","吳音寧","管中閔","拔管","挺管","","深澳","燃煤","蔣萬安","張顯耀","柯文哲","悠遊卡","陳景峻","賴清德","丁守中",
"轉型正義","賴揆","習近平","林佳龍","呂秀蓮","涂醒哲", "世大運", "段宜康", "蔡英文", "馬英九", "韓國瑜" , "中執會" , "三立" , "中天", "姚文智" ,
"柯P" , "陳佩琪" , "台北市長", "前總統", "前副總統", "副總統" , "蔡璧如", "金溥聰","台灣價值", "勤政清廉愛鄉土", "意識形態", "月票", "雙城論壇",
"陳昭南", "侯友宜", "重陽", "敬老", "建管處", "深澳電廠" , "陳彥伯", "燈節", "燈節廠商", "綠營", "藍營")
write(new_stop_words,file = "new_stop_words.txt",sep="\n")
new_stop_words <- c("柯建銘","吳敦義","吳茂昆","吳音寧","管中閔","拔管","挺管","深澳","燃煤","蔣萬安","張顯耀","柯文哲","悠遊卡","陳景峻","賴清德","丁守中",
"轉型正義","賴揆","習近平","林佳龍","呂秀蓮","涂醒哲", "世大運", "段宜康", "蔡英文", "馬英九", "韓國瑜" , "中執會" , "三立" , "中天", "姚文智" ,
"柯P" , "陳佩琪" , "台北市長", "前總統", "前副總統", "副總統" , "蔡璧如", "金溥聰","台灣價值", "勤政清廉愛鄉土", "意識形態", "月票", "雙城論壇",
"陳昭南", "侯友宜", "重陽", "敬老", "建管處", "深澳電廠" , "陳彥伯", "燈節", "燈節廠商", "綠營", "藍營")
write(new_stop_words,file = "new_stop_words.txt",sep="\n")
setwd("~/Documents/GitHub/courseR/final_project/Appnews")
Di_MayAppnews <- read.csv("Di_MayAppnews")
View(Di_MayAppnews)
sum(is.na(Di_MayAppnews$time))
86/236
library(rvest)
doc <- read_html(url)
url <- "https://tw.appledaily.com/new/realtime/20180502/1345785/"
css  <- "#article > div.wrapper > div > main > article > div > div.ndArticle_contentBox > article > div > p"
doc <- read_html(url)
node <- html_nodes(doc, css)
text <- html_text(node)
Di_AprAppnews <- read.csv("Di_AprAppnews")
View(Di_AprAppnews)
Di_MayAppnews <- read.csv("Di_MayAppnews")
Di_AprAppnews <- read.csv("Di_AprAppnews")
Di_MarAppnews <- read.csv("Di_MarAppnews")
Di_FebAppnews <- read.csv("Di_FebAppnews")
Di_JanAppnews <- read.csv("Di_JanAppnews")
Di_Appnews <- rbind(Di_MayAppnews, Di_AprAppnews, Di_MarAppnews, Di_FebAppnews, Di_JanAppnews)
View(Di_Appnews)
sum(is.na(Di_Appnews$time))
153/443
library(magrittr)
Di_Appnews <- Di_Appnews %>% na.omit()
443-153
strsplit(Di_Appnews$time[1], "報導)")
typeof(Di_Appnews$time[1])
strsplit(Di_Appnews$time[1] %>% as.character(), "報導)")
t <- strsplit(Di_Appnews$time[1] %>% as.character(), "報導)")
View(t)
t[[1]]
t <- strsplit(Di_Appnews$time[1] %>% as.character(), "報導）")
t[1]
t[1][1]
View(t)
t[[1]][1]
b <- strsplit(Di_Appnews$time[8] %>% as.character(), "報導）")
b[[1]][1]
typeof(b[[1]][1])
t <- strsplit(Di_Appnews$time[1] %>% as.character(), "報導）")
cleancontent1 <- t[[1]][1]
tt <- strsplit(Di_Appnews$time[2] %>% as.character(), "報導）")
cleancontent2 <- tt[[1]][1]
cleancontent <- rbind(cleancontent1, cleancontent2)
View(cleancontent)
cleancontent <- c()
for( i in 1:length(Di_Appnews$time)){
t <- strsplit(Di_Appnews$time[1] %>% as.character())
t <- t[[1]][1]
cleancontent <- rbind(cleancontent, t)
print(i)
}
for( i in 1:length(Di_Appnews$time)){
t <- strsplit(Di_Appnews$time[1] %>% as.character() , "報導）")
t <- t[[1]][1]
cleancontent <- rbind(cleancontent, t)
print(i)
}
View(cleancontent)
cleancontent <- c()
for( i in 1:length(Di_Appnews$time)){
t <- strsplit(Di_Appnews$time[i] %>% as.character() , "報導）")
t <- t[[1]][1]
cleancontent <- rbind(cleancontent, t)
print(i)
}
View(cleancontent)
#切割時間
Di_Appnews <- Di_Appnews %>% separate(text, c("出版時間","time"),":") #將text的欄位切成出版時間和time兩欄欄
View(Di_Appnews)
#切割時間
Di_Appnews <- Di_Appnews %>% separate(出版時間, c("出版時間","time"),"：") #將text的欄位切成出版時間和time兩欄欄
Di_MayAppnews <- read.csv("Di_MayAppnews")
Di_AprAppnews <- read.csv("Di_AprAppnews")
Di_MarAppnews <- read.csv("Di_MarAppnews")
Di_FebAppnews <- read.csv("Di_FebAppnews")
Di_JanAppnews <- read.csv("Di_JanAppnews")
Di_Appnews <- rbind(Di_MayAppnews, Di_AprAppnews, Di_MarAppnews, Di_FebAppnews, Di_JanAppnews)
sum(is.na(Di_Appnews$time)) #QQ
#去掉含有NA值的欄位QQ
Di_Appnews <- Di_Appnews %>% na.omit()
#用strsplit將 報導） 後面的雜訊清掉
cleancontent <- c()
for( i in 1:length(Di_Appnews$time)){
t <- strsplit(Di_Appnews$time[i] %>% as.character() , "報導）")
t <- t[[1]][1]
cleancontent <- rbind(cleancontent, t)
print(i)
}
#切割時間
Di <- Di_Appnews %>% separate(出版時間, c("出版時間","time"),"：") #將text的欄位切成出版時間和time兩欄欄
#切割時間
Di <- Di_Appnews %>% separate(text, c("出版時間","time"),"：") #將text的欄位切成出版時間和time兩欄欄
View(Di)
library(rvest)
library(magrittr)
Di_MayAppnews <- read.csv("Di_MayAppnews")
Di_AprAppnews <- read.csv("Di_AprAppnews")
Di_MarAppnews <- read.csv("Di_MarAppnews")
Di_FebAppnews <- read.csv("Di_FebAppnews")
Di_JanAppnews <- read.csv("Di_JanAppnews")
Di_Appnews <- rbind(Di_MayAppnews, Di_AprAppnews, Di_MarAppnews, Di_FebAppnews, Di_JanAppnews)
#去掉含有NA值的欄位QQ
Di_Appnews <- Di_Appnews %>% na.omit()
#用strsplit將 報導） 後面的雜訊清掉
cleancontent <- c()
for( i in 1:length(Di_Appnews$time)){
t <- strsplit(Di_Appnews$time[i] %>% as.character() , "報導）")
t <- t[[1]][1]
cleancontent <- rbind(cleancontent, t)
print(i)
}
#切割時間
Di <- Di_Appnews %>% separate(text, c("出版時間","ttime"),"：")
View(Di)
#將text的欄位切成出版時間和time兩欄欄
Di <- Di %>% separate(ttime, c("ttime","stime")," ")
View(Di)
Di <- Di %>% separate(ttime, c("year","month", "day"),"/")
View(Di)
#切割時間
Di_Appnews <- Di_Appnews %>% separate(text, c("出版時間","ttime"),"：")
#將text的欄位切成出版時間和time兩欄欄
Di_Appnews <- Di_Appnews %>% separate(ttime, c("ttime","stime")," ")
Di_Appnews <- Di_Appnews %>% separate(ttime, c("year","month", "day"),"/")
View(Di_Appnews)
Ding_Appnews <- cbind(Di_Appnews$title, Di_Appnews$year, Di_Appnews$month, Di_Appnews$day, cleancontent)
View(Ding_Appnews)
cleancontent <- data.frame(cleancontent)
Ding <- cbind(Di_Appnews$title, Di_Appnews$year, Di_Appnews$month, Di_Appnews$day, cleancontent)
View(Ding)
Ding_Appnews <- cbind(Di_Appnews$title, Di_Appnews$year, Di_Appnews$month, Di_Appnews$day, cleancontent)
View(Ding_Appnews)
colnames(Ding_Appnews) <- c("title", "year", "month", "day", "content")
View(Ding_Appnews)
#增加Media的欄位
Media <- c()
text <- c("Apple")
for( i in 1:length(Ding_Appnews$content)){
Media <- rbind(Media, text)
}
Media <- Media %>% data.frame()
View(Media)
#增加Media的欄位
Media <- c()
text <- c("Apple")
for( i in 1:length(Ding_Appnews$content)){
Media <- rbind(Media, text)
}
Ding_Appnews <- rbind(Media, Ding_Appnews)
Ding_Appnews <- cbind(Media, Ding_Appnews)
View(Ding_Appnews)
write.csv(Ding_Appnews, "Ding_Appnews")
library(rvest)
library(magrittr)
Yao_MayAppnews <- read.csv("Yao_MayAppnews")
Yao_AprAppnews <- read.csv("Yao_AprAppnews")
Yao_MarAppnews <- read.csv("Yao_MarAppnews")
Yao_FebAppnews <- read.csv("Yao_FebAppnews")
Yao_JanAppnews <- read.csv("Yao_JanAppnews")
Yao_JanAppnews <- read.csv("Yao_JanAppnews")
library(rvest)
library(magrittr)
library(httr)
# #爬姚文智五月的新聞
#爬全部姚文智五月新聞搜尋頁面的連結
url <- "https://tw.appledaily.com/search"
#爬五月連結的每一篇新聞
#先寫可以爬每一篇新聞時間標題內文的function
FindAppnews <- function(URL){
doc <- read_html(URL %>% as.character())
title <- html_text(html_node(doc,  "#article > div.wrapper > div > main > article > hgroup > h1"))
time <- html_text(html_node(doc, "#article > div.wrapper > div > main > article > hgroup > div.ndArticle_creat"))
text <- html_text(html_node(doc, "#article > div.wrapper > div > main > article > div > div.ndArticle_contentBox > article > div > p"))
output <- data.frame( title = title,
text = text,
time = time)
return(output)
}
# #爬姚文智一月的新聞
#爬全部姚文智一月新聞搜尋頁面的連結
url <- "https://tw.appledaily.com/search"
Yao_JanApplinks <- c()
#爬全部連結的迴圈
for(p in 1:5){
form <- list(searchType = "text",
keyword = "姚文智",
totalpage = "41",
page = sprintf("%d" , p ),
sorttype = "1",
rangedate = "[20180101 TO 20180131999:99]")
res <- POST(url, body = form)
doc.str <- content(res, "text")
doc <- read_html(doc.str)
css <- "#result > li > div > h2 > a"
links <- html_attr(html_nodes(doc,css), "href")
Yao_JanApplinks <- c(Yao_JanApplinks, links)
message(p)
}
#test是否抓到重複的網址
test <- Yao_JanApplinks[?duplicated(Yao_JanApplinks)]
#爬一月連結的每一篇新聞
#爬姚文智一月的每一篇新聞的時間標題內文
Yao_JanAppnews <- data.frame()
for ( t in 1:length(Yao_JanApplinks)) {
print(t)
output4 <- FindAppnews(Yao_JanApplinks[t])
YaoJanAppnews <- rbind(Yao_JanAppnews, output4)
Sys.sleep(sample(1:10, 1))
}
#爬全部連結的迴圈
for(p in 1:5){
form <- list(searchType = "text",
keyword = "姚文智",
totalpage = "41",
page = sprintf("%d" , p ),
sorttype = "1",
rangedate = "[20180101 TO 20180131999:99]")
res <- POST(url, body = form)
doc.str <- content(res, "text")
doc <- read_html(doc.str)
css <- "#result > li > div > h2 > a"
links <- html_attr(html_nodes(doc,css), "href")
Yao_JanApplinks <- c(Yao_JanApplinks, links)
message(p)
}
s <- read.csv("Ko_JanAppnews")
View(s)
